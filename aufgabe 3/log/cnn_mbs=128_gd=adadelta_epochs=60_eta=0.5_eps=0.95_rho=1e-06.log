Loading MNIST dataset...
Creating network...
Using adadelta for updates with learning rate: 0.5, epsilon: 0.95, rho: 1e-06
Starting training...
Epoch 1 of 60 took 2.133s
  training loss:		0.038441
Epoch 2 of 60 took 2.072s
  training loss:		0.012902
Epoch 3 of 60 took 2.074s
  training loss:		0.009459
Epoch 4 of 60 took 2.073s
  training loss:		0.007874
Epoch 5 of 60 took 2.077s
  training loss:		0.006922
Epoch 6 of 60 took 2.075s
  training loss:		0.006189
Epoch 7 of 60 took 2.069s
  training loss:		0.005752
Epoch 8 of 60 took 2.069s
  training loss:		0.005343
Epoch 9 of 60 took 2.072s
  training loss:		0.005025
Epoch 10 of 60 took 2.073s
  training loss:		0.004737
Epoch 11 of 60 took 2.075s
  training loss:		0.004501
Epoch 12 of 60 took 2.073s
  training loss:		0.004291
Epoch 13 of 60 took 2.073s
  training loss:		0.004132
Epoch 14 of 60 took 2.074s
  training loss:		0.003968
Epoch 15 of 60 took 2.072s
  training loss:		0.003839
Epoch 16 of 60 took 2.075s
  training loss:		0.003691
Epoch 17 of 60 took 2.072s
  training loss:		0.003600
Epoch 18 of 60 took 2.075s
  training loss:		0.003497
Epoch 19 of 60 took 2.074s
  training loss:		0.003382
Epoch 20 of 60 took 2.073s
  training loss:		0.003294
Epoch 21 of 60 took 2.072s
  training loss:		0.003209
Epoch 22 of 60 took 2.073s
  training loss:		0.003136
Epoch 23 of 60 took 2.075s
  training loss:		0.003057
Epoch 24 of 60 took 2.070s
  training loss:		0.002976
Epoch 25 of 60 took 2.073s
  training loss:		0.002918
Epoch 26 of 60 took 2.071s
  training loss:		0.002870
Epoch 27 of 60 took 2.075s
  training loss:		0.002803
Epoch 28 of 60 took 2.076s
  training loss:		0.002745
Epoch 29 of 60 took 2.073s
  training loss:		0.002698
Epoch 30 of 60 took 2.072s
  training loss:		0.002647
Epoch 31 of 60 took 2.072s
  training loss:		0.002600
Epoch 32 of 60 took 2.071s
  training loss:		0.002556
Epoch 33 of 60 took 2.074s
  training loss:		0.002519
Epoch 34 of 60 took 2.071s
  training loss:		0.002468
Epoch 35 of 60 took 2.074s
  training loss:		0.002419
Epoch 36 of 60 took 2.072s
  training loss:		0.002402
Epoch 37 of 60 took 2.072s
  training loss:		0.002361
Epoch 38 of 60 took 2.073s
  training loss:		0.002318
Epoch 39 of 60 took 2.072s
  training loss:		0.002275
Epoch 40 of 60 took 2.075s
  training loss:		0.002245
Epoch 41 of 60 took 2.072s
  training loss:		0.002224
Epoch 42 of 60 took 2.070s
  training loss:		0.002198
Epoch 43 of 60 took 2.073s
  training loss:		0.002151
Epoch 44 of 60 took 2.072s
  training loss:		0.002131
Epoch 45 of 60 took 2.072s
  training loss:		0.002099
Epoch 46 of 60 took 2.071s
  training loss:		0.002058
Epoch 47 of 60 took 2.075s
  training loss:		0.002040
Epoch 48 of 60 took 2.072s
  training loss:		0.002027
Epoch 49 of 60 took 2.067s
  training loss:		0.001991
Epoch 50 of 60 took 2.072s
  training loss:		0.001957
Epoch 51 of 60 took 2.070s
  training loss:		0.001943
Epoch 52 of 60 took 2.072s
  training loss:		0.001916
Epoch 53 of 60 took 2.074s
  training loss:		0.001884
Epoch 54 of 60 took 2.072s
  training loss:		0.001865
Epoch 55 of 60 took 2.067s
  training loss:		0.001845
Epoch 56 of 60 took 2.069s
  training loss:		0.001813
Epoch 57 of 60 took 2.072s
  training loss:		0.001813
Epoch 58 of 60 took 2.073s
  training loss:		0.001780
Epoch 59 of 60 took 2.072s
  training loss:		0.001759
Epoch 60 of 60 took 2.072s
  training loss:		0.001726
Training accuracy:		99.22 %
Final results:
  test loss:			0.002347
  test accuracy:		98.97 %
