Loading MNIST dataset...
Creating network...
Using momentum for updates with learning rate: 0.5, momentum: 0.9
Starting training...
Epoch 1 of 60 took 3.567s
  training loss:		0.100084
Epoch 2 of 60 took 3.517s
  training loss:		0.100000
Epoch 3 of 60 took 3.516s
  training loss:		0.100000
Epoch 4 of 60 took 3.515s
  training loss:		0.100000
Epoch 5 of 60 took 3.512s
  training loss:		0.100000
Epoch 6 of 60 took 3.512s
  training loss:		0.100000
Epoch 7 of 60 took 3.512s
  training loss:		0.100000
Epoch 8 of 60 took 3.512s
  training loss:		0.100000
Epoch 9 of 60 took 3.513s
  training loss:		0.100000
Epoch 10 of 60 took 3.513s
  training loss:		0.100000
Epoch 11 of 60 took 3.514s
  training loss:		0.100000
Epoch 12 of 60 took 3.516s
  training loss:		0.100000
Epoch 13 of 60 took 3.512s
  training loss:		0.100000
Epoch 14 of 60 took 3.513s
  training loss:		0.100000
Epoch 15 of 60 took 3.580s
  training loss:		0.100000
Epoch 16 of 60 took 3.669s
  training loss:		0.100000
Epoch 17 of 60 took 3.525s
  training loss:		0.100000
Epoch 18 of 60 took 3.602s
  training loss:		0.100000
Epoch 19 of 60 took 3.522s
  training loss:		0.100000
Epoch 20 of 60 took 3.605s
  training loss:		0.100000
Epoch 21 of 60 took 3.688s
  training loss:		0.100000
Epoch 22 of 60 took 3.632s
  training loss:		0.100000
Epoch 23 of 60 took 3.510s
  training loss:		0.100000
Epoch 24 of 60 took 3.510s
  training loss:		0.100000
Epoch 25 of 60 took 3.607s
  training loss:		0.100000
Epoch 26 of 60 took 3.571s
  training loss:		0.100000
Epoch 27 of 60 took 3.681s
  training loss:		0.100000
Epoch 28 of 60 took 3.518s
  training loss:		0.100000
Epoch 29 of 60 took 3.514s
  training loss:		0.100000
Epoch 30 of 60 took 3.512s
  training loss:		0.100000
Epoch 31 of 60 took 3.510s
  training loss:		0.100000
Epoch 32 of 60 took 3.510s
  training loss:		0.100000
Epoch 33 of 60 took 3.509s
  training loss:		0.100000
Epoch 34 of 60 took 3.508s
  training loss:		0.100000
Epoch 35 of 60 took 3.602s
  training loss:		0.100000
Epoch 36 of 60 took 3.509s
  training loss:		0.100000
Epoch 37 of 60 took 3.507s
  training loss:		0.100000
Epoch 38 of 60 took 3.509s
  training loss:		0.100000
Epoch 39 of 60 took 3.508s
  training loss:		0.100000
Epoch 40 of 60 took 3.509s
  training loss:		0.100000
Epoch 41 of 60 took 3.506s
  training loss:		0.100000
Epoch 42 of 60 took 3.509s
  training loss:		0.100000
Epoch 43 of 60 took 3.509s
  training loss:		0.100000
Epoch 44 of 60 took 3.508s
  training loss:		0.100000
Epoch 45 of 60 took 3.508s
  training loss:		0.100000
Epoch 46 of 60 took 3.511s
  training loss:		0.100000
Epoch 47 of 60 took 3.504s
  training loss:		0.100000
Epoch 48 of 60 took 3.505s
  training loss:		0.100000
Epoch 49 of 60 took 3.637s
  training loss:		0.100000
Epoch 50 of 60 took 3.523s
  training loss:		0.100000
Epoch 51 of 60 took 3.505s
  training loss:		0.100000
Epoch 52 of 60 took 3.508s
  training loss:		0.100000
Epoch 53 of 60 took 3.505s
  training loss:		0.100000
Epoch 54 of 60 took 3.505s
  training loss:		0.100000
Epoch 55 of 60 took 3.507s
  training loss:		0.100000
Epoch 56 of 60 took 3.505s
  training loss:		0.100000
Epoch 57 of 60 took 3.507s
  training loss:		0.100000
Epoch 58 of 60 took 3.508s
  training loss:		0.100000
Epoch 59 of 60 took 3.507s
  training loss:		0.100000
Epoch 60 of 60 took 3.505s
  training loss:		0.100000
Training accuracy:		10.42 %
Final results:
  test loss:			0.100000
  test accuracy:		9.72 %
