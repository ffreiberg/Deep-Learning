Loading MNIST dataset...
Creating network...
Using adadelta for updates with learning rate: 0.01, epsilon: 0.95, rho: 1e-06
Starting training...
Epoch 1 of 60 took 2.327s
  training loss:		0.131183
Epoch 2 of 60 took 2.242s
  training loss:		0.090524
Epoch 3 of 60 took 2.237s
  training loss:		0.089575
Epoch 4 of 60 took 2.238s
  training loss:		0.088601
Epoch 5 of 60 took 2.238s
  training loss:		0.087477
Epoch 6 of 60 took 2.242s
  training loss:		0.086088
Epoch 7 of 60 took 2.242s
  training loss:		0.084261
Epoch 8 of 60 took 2.241s
  training loss:		0.081754
Epoch 9 of 60 took 2.240s
  training loss:		0.078380
Epoch 10 of 60 took 2.239s
  training loss:		0.074173
Epoch 11 of 60 took 2.237s
  training loss:		0.069371
Epoch 12 of 60 took 2.238s
  training loss:		0.064239
Epoch 13 of 60 took 2.237s
  training loss:		0.059117
Epoch 14 of 60 took 2.240s
  training loss:		0.054411
Epoch 15 of 60 took 2.239s
  training loss:		0.050313
Epoch 16 of 60 took 2.236s
  training loss:		0.046770
Epoch 17 of 60 took 2.239s
  training loss:		0.043664
Epoch 18 of 60 took 2.240s
  training loss:		0.040909
Epoch 19 of 60 took 2.238s
  training loss:		0.038519
Epoch 20 of 60 took 2.239s
  training loss:		0.036397
Epoch 21 of 60 took 2.238s
  training loss:		0.034558
Epoch 22 of 60 took 2.238s
  training loss:		0.032950
Epoch 23 of 60 took 2.241s
  training loss:		0.031531
Epoch 24 of 60 took 2.237s
  training loss:		0.030292
Epoch 25 of 60 took 2.238s
  training loss:		0.029196
Epoch 26 of 60 took 2.241s
  training loss:		0.028199
Epoch 27 of 60 took 2.236s
  training loss:		0.027291
Epoch 28 of 60 took 2.251s
  training loss:		0.026492
Epoch 29 of 60 took 2.274s
  training loss:		0.025728
Epoch 30 of 60 took 2.275s
  training loss:		0.025047
Epoch 31 of 60 took 2.267s
  training loss:		0.024403
Epoch 32 of 60 took 2.193s
  training loss:		0.023829
Epoch 33 of 60 took 2.084s
  training loss:		0.023270
Epoch 34 of 60 took 2.107s
  training loss:		0.022743
Epoch 35 of 60 took 2.068s
  training loss:		0.022259
Epoch 36 of 60 took 2.067s
  training loss:		0.021792
Epoch 37 of 60 took 2.066s
  training loss:		0.021334
Epoch 38 of 60 took 2.064s
  training loss:		0.020923
Epoch 39 of 60 took 2.064s
  training loss:		0.020516
Epoch 40 of 60 took 2.064s
  training loss:		0.020140
Epoch 41 of 60 took 2.079s
  training loss:		0.019792
Epoch 42 of 60 took 2.065s
  training loss:		0.019441
Epoch 43 of 60 took 2.105s
  training loss:		0.019109
Epoch 44 of 60 took 2.067s
  training loss:		0.018801
Epoch 45 of 60 took 2.066s
  training loss:		0.018497
Epoch 46 of 60 took 2.065s
  training loss:		0.018217
Epoch 47 of 60 took 2.065s
  training loss:		0.017936
Epoch 48 of 60 took 2.064s
  training loss:		0.017685
Epoch 49 of 60 took 2.062s
  training loss:		0.017439
Epoch 50 of 60 took 2.105s
  training loss:		0.017182
Epoch 51 of 60 took 2.093s
  training loss:		0.016961
Epoch 52 of 60 took 2.107s
  training loss:		0.016737
Epoch 53 of 60 took 2.081s
  training loss:		0.016526
Epoch 54 of 60 took 2.066s
  training loss:		0.016308
Epoch 55 of 60 took 2.116s
  training loss:		0.016102
Epoch 56 of 60 took 2.135s
  training loss:		0.015917
Epoch 57 of 60 took 2.064s
  training loss:		0.015734
Epoch 58 of 60 took 2.063s
  training loss:		0.015550
Epoch 59 of 60 took 2.077s
  training loss:		0.015360
Epoch 60 of 60 took 2.063s
  training loss:		0.015195
Training accuracy:		92.53 %
Final results:
  test loss:			0.014094
  test accuracy:		93.41 %
