Loading MNIST dataset...
Creating network...
Using adadelta for updates with learning rate: 0.5, epsilon: 0.95, rho: 1e-06
Starting training...
Epoch 1 of 60 took 1.968s
  training loss:		0.082322
Epoch 2 of 60 took 1.931s
  training loss:		0.040558
Epoch 3 of 60 took 1.929s
  training loss:		0.027196
Epoch 4 of 60 took 1.932s
  training loss:		0.020012
Epoch 5 of 60 took 1.931s
  training loss:		0.016513
Epoch 6 of 60 took 1.933s
  training loss:		0.014359
Epoch 7 of 60 took 1.931s
  training loss:		0.012869
Epoch 8 of 60 took 1.931s
  training loss:		0.011776
Epoch 9 of 60 took 1.936s
  training loss:		0.010832
Epoch 10 of 60 took 1.938s
  training loss:		0.010055
Epoch 11 of 60 took 1.935s
  training loss:		0.009450
Epoch 12 of 60 took 1.936s
  training loss:		0.008952
Epoch 13 of 60 took 1.938s
  training loss:		0.008469
Epoch 14 of 60 took 1.934s
  training loss:		0.008055
Epoch 15 of 60 took 1.938s
  training loss:		0.007718
Epoch 16 of 60 took 1.936s
  training loss:		0.007413
Epoch 17 of 60 took 1.937s
  training loss:		0.007140
Epoch 18 of 60 took 1.934s
  training loss:		0.006897
Epoch 19 of 60 took 1.936s
  training loss:		0.006671
Epoch 20 of 60 took 1.935s
  training loss:		0.006483
Epoch 21 of 60 took 1.934s
  training loss:		0.006347
Epoch 22 of 60 took 1.939s
  training loss:		0.006144
Epoch 23 of 60 took 1.934s
  training loss:		0.005984
Epoch 24 of 60 took 1.933s
  training loss:		0.005844
Epoch 25 of 60 took 1.934s
  training loss:		0.005744
Epoch 26 of 60 took 1.938s
  training loss:		0.005588
Epoch 27 of 60 took 1.939s
  training loss:		0.005491
Epoch 28 of 60 took 1.934s
  training loss:		0.005380
Epoch 29 of 60 took 1.933s
  training loss:		0.005275
Epoch 30 of 60 took 1.933s
  training loss:		0.005195
Epoch 31 of 60 took 1.936s
  training loss:		0.005131
Epoch 32 of 60 took 1.935s
  training loss:		0.005028
Epoch 33 of 60 took 1.935s
  training loss:		0.004939
Epoch 34 of 60 took 1.937s
  training loss:		0.004868
Epoch 35 of 60 took 1.937s
  training loss:		0.004790
Epoch 36 of 60 took 1.939s
  training loss:		0.004735
Epoch 37 of 60 took 1.938s
  training loss:		0.004676
Epoch 38 of 60 took 1.936s
  training loss:		0.004612
Epoch 39 of 60 took 1.935s
  training loss:		0.004555
Epoch 40 of 60 took 1.937s
  training loss:		0.004512
Epoch 41 of 60 took 1.937s
  training loss:		0.004444
Epoch 42 of 60 took 1.937s
  training loss:		0.004379
Epoch 43 of 60 took 1.938s
  training loss:		0.004328
Epoch 44 of 60 took 1.939s
  training loss:		0.004270
Epoch 45 of 60 took 1.938s
  training loss:		0.004222
Epoch 46 of 60 took 1.936s
  training loss:		0.004170
Epoch 47 of 60 took 1.935s
  training loss:		0.004117
Epoch 48 of 60 took 1.935s
  training loss:		0.004088
Epoch 49 of 60 took 1.936s
  training loss:		0.004033
Epoch 50 of 60 took 1.937s
  training loss:		0.003959
Epoch 51 of 60 took 1.943s
  training loss:		0.003988
Epoch 52 of 60 took 1.943s
  training loss:		0.003913
Epoch 53 of 60 took 1.944s
  training loss:		0.003878
Epoch 54 of 60 took 1.941s
  training loss:		0.003825
Epoch 55 of 60 took 1.939s
  training loss:		0.003834
Epoch 56 of 60 took 1.941s
  training loss:		0.003783
Epoch 57 of 60 took 1.942s
  training loss:		0.003733
Epoch 58 of 60 took 1.945s
  training loss:		0.003712
Epoch 59 of 60 took 1.943s
  training loss:		0.003681
Epoch 60 of 60 took 1.946s
  training loss:		0.003675
Training accuracy:		98.36 %
Final results:
  test loss:			0.003655
  test accuracy:		98.44 %
