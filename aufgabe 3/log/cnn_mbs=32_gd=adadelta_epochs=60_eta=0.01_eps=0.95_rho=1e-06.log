Loading MNIST dataset...
Creating network...
Using adadelta for updates with learning rate: 0.01, epsilon: 0.95, rho: 1e-06
Starting training...
Epoch 1 of 60 took 2.793s
  training loss:		0.098363
Epoch 2 of 60 took 2.734s
  training loss:		0.080968
Epoch 3 of 60 took 2.734s
  training loss:		0.063380
Epoch 4 of 60 took 2.735s
  training loss:		0.043486
Epoch 5 of 60 took 2.733s
  training loss:		0.033662
Epoch 6 of 60 took 2.735s
  training loss:		0.028745
Epoch 7 of 60 took 2.733s
  training loss:		0.025653
Epoch 8 of 60 took 2.734s
  training loss:		0.023415
Epoch 9 of 60 took 2.736s
  training loss:		0.021661
Epoch 10 of 60 took 2.736s
  training loss:		0.020200
Epoch 11 of 60 took 2.736s
  training loss:		0.018970
Epoch 12 of 60 took 2.734s
  training loss:		0.017915
Epoch 13 of 60 took 2.734s
  training loss:		0.016983
Epoch 14 of 60 took 2.729s
  training loss:		0.016169
Epoch 15 of 60 took 2.730s
  training loss:		0.015446
Epoch 16 of 60 took 2.731s
  training loss:		0.014789
Epoch 17 of 60 took 2.730s
  training loss:		0.014205
Epoch 18 of 60 took 2.731s
  training loss:		0.013662
Epoch 19 of 60 took 2.730s
  training loss:		0.013164
Epoch 20 of 60 took 2.729s
  training loss:		0.012714
Epoch 21 of 60 took 2.732s
  training loss:		0.012296
Epoch 22 of 60 took 2.732s
  training loss:		0.011921
Epoch 23 of 60 took 2.730s
  training loss:		0.011554
Epoch 24 of 60 took 2.732s
  training loss:		0.011233
Epoch 25 of 60 took 2.731s
  training loss:		0.010924
Epoch 26 of 60 took 2.731s
  training loss:		0.010648
Epoch 27 of 60 took 2.728s
  training loss:		0.010380
Epoch 28 of 60 took 2.730s
  training loss:		0.010148
Epoch 29 of 60 took 2.730s
  training loss:		0.009913
Epoch 30 of 60 took 2.730s
  training loss:		0.009701
Epoch 31 of 60 took 2.729s
  training loss:		0.009508
Epoch 32 of 60 took 2.731s
  training loss:		0.009314
Epoch 33 of 60 took 2.732s
  training loss:		0.009132
Epoch 34 of 60 took 2.731s
  training loss:		0.008966
Epoch 35 of 60 took 2.731s
  training loss:		0.008804
Epoch 36 of 60 took 2.731s
  training loss:		0.008656
Epoch 37 of 60 took 2.732s
  training loss:		0.008514
Epoch 38 of 60 took 2.728s
  training loss:		0.008385
Epoch 39 of 60 took 2.730s
  training loss:		0.008248
Epoch 40 of 60 took 2.730s
  training loss:		0.008131
Epoch 41 of 60 took 2.730s
  training loss:		0.008012
Epoch 42 of 60 took 2.729s
  training loss:		0.007900
Epoch 43 of 60 took 2.731s
  training loss:		0.007785
Epoch 44 of 60 took 2.730s
  training loss:		0.007688
Epoch 45 of 60 took 2.729s
  training loss:		0.007593
Epoch 46 of 60 took 2.730s
  training loss:		0.007496
Epoch 47 of 60 took 2.731s
  training loss:		0.007403
Epoch 48 of 60 took 2.730s
  training loss:		0.007323
Epoch 49 of 60 took 2.729s
  training loss:		0.007222
Epoch 50 of 60 took 2.732s
  training loss:		0.007151
Epoch 51 of 60 took 2.730s
  training loss:		0.007071
Epoch 52 of 60 took 2.730s
  training loss:		0.007000
Epoch 53 of 60 took 2.730s
  training loss:		0.006921
Epoch 54 of 60 took 2.728s
  training loss:		0.006853
Epoch 55 of 60 took 2.730s
  training loss:		0.006782
Epoch 56 of 60 took 2.732s
  training loss:		0.006715
Epoch 57 of 60 took 2.732s
  training loss:		0.006648
Epoch 58 of 60 took 2.731s
  training loss:		0.006591
Epoch 59 of 60 took 2.731s
  training loss:		0.006526
Epoch 60 of 60 took 2.730s
  training loss:		0.006473
Training accuracy:		96.86 %
Final results:
  test loss:			0.005850
  test accuracy:		97.46 %
