Loading MNIST dataset...
Creating network...
Using adadelta for updates with learning rate: 0.5, epsilon: 0.95, rho: 1e-06
Starting training...
Epoch 1 of 60 took 2.022s
  training loss:		0.061546
Epoch 2 of 60 took 1.978s
  training loss:		0.023462
Epoch 3 of 60 took 1.978s
  training loss:		0.015156
Epoch 4 of 60 took 1.982s
  training loss:		0.011898
Epoch 5 of 60 took 1.978s
  training loss:		0.010075
Epoch 6 of 60 took 1.979s
  training loss:		0.008902
Epoch 7 of 60 took 1.981s
  training loss:		0.008028
Epoch 8 of 60 took 1.978s
  training loss:		0.007376
Epoch 9 of 60 took 1.982s
  training loss:		0.006832
Epoch 10 of 60 took 1.981s
  training loss:		0.006420
Epoch 11 of 60 took 1.982s
  training loss:		0.006104
Epoch 12 of 60 took 1.983s
  training loss:		0.005799
Epoch 13 of 60 took 1.981s
  training loss:		0.005513
Epoch 14 of 60 took 1.981s
  training loss:		0.005329
Epoch 15 of 60 took 1.981s
  training loss:		0.005130
Epoch 16 of 60 took 1.984s
  training loss:		0.004957
Epoch 17 of 60 took 1.980s
  training loss:		0.004823
Epoch 18 of 60 took 1.979s
  training loss:		0.004667
Epoch 19 of 60 took 1.981s
  training loss:		0.004514
Epoch 20 of 60 took 1.982s
  training loss:		0.004444
Epoch 21 of 60 took 1.981s
  training loss:		0.004328
Epoch 22 of 60 took 1.983s
  training loss:		0.004216
Epoch 23 of 60 took 1.979s
  training loss:		0.004135
Epoch 24 of 60 took 1.980s
  training loss:		0.004035
Epoch 25 of 60 took 1.981s
  training loss:		0.003967
Epoch 26 of 60 took 1.982s
  training loss:		0.003884
Epoch 27 of 60 took 1.984s
  training loss:		0.003819
Epoch 28 of 60 took 1.985s
  training loss:		0.003729
Epoch 29 of 60 took 1.984s
  training loss:		0.003658
Epoch 30 of 60 took 1.982s
  training loss:		0.003620
Epoch 31 of 60 took 1.982s
  training loss:		0.003580
Epoch 32 of 60 took 1.984s
  training loss:		0.003495
Epoch 33 of 60 took 1.984s
  training loss:		0.003444
Epoch 34 of 60 took 1.986s
  training loss:		0.003409
Epoch 35 of 60 took 1.984s
  training loss:		0.003341
Epoch 36 of 60 took 1.984s
  training loss:		0.003311
Epoch 37 of 60 took 1.984s
  training loss:		0.003261
Epoch 38 of 60 took 1.981s
  training loss:		0.003236
Epoch 39 of 60 took 1.979s
  training loss:		0.003167
Epoch 40 of 60 took 1.977s
  training loss:		0.003127
Epoch 41 of 60 took 1.979s
  training loss:		0.003072
Epoch 42 of 60 took 1.982s
  training loss:		0.003068
Epoch 43 of 60 took 1.983s
  training loss:		0.003032
Epoch 44 of 60 took 1.984s
  training loss:		0.002985
Epoch 45 of 60 took 1.978s
  training loss:		0.002936
Epoch 46 of 60 took 1.981s
  training loss:		0.002918
Epoch 47 of 60 took 1.979s
  training loss:		0.002882
Epoch 48 of 60 took 1.979s
  training loss:		0.002858
Epoch 49 of 60 took 1.981s
  training loss:		0.002829
Epoch 50 of 60 took 1.982s
  training loss:		0.002780
Epoch 51 of 60 took 1.981s
  training loss:		0.002765
Epoch 52 of 60 took 1.980s
  training loss:		0.002763
Epoch 53 of 60 took 1.979s
  training loss:		0.002729
Epoch 54 of 60 took 1.979s
  training loss:		0.002695
Epoch 55 of 60 took 1.982s
  training loss:		0.002665
Epoch 56 of 60 took 1.981s
  training loss:		0.002624
Epoch 57 of 60 took 1.979s
  training loss:		0.002609
Epoch 58 of 60 took 1.982s
  training loss:		0.002591
Epoch 59 of 60 took 1.980s
  training loss:		0.002565
Epoch 60 of 60 took 1.987s
  training loss:		0.002557
Training accuracy:		98.83 %
Final results:
  test loss:			0.002836
  test accuracy:		98.86 %
