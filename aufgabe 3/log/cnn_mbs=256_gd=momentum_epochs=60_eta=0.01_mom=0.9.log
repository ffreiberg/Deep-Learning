Loading MNIST dataset...
Creating network...
Using momentum for updates with learning rate: 0.01, momentum: 0.9
Starting training...
Epoch 1 of 60 took 2.078s
  training loss:		0.105274
Epoch 2 of 60 took 2.004s
  training loss:		0.075684
Epoch 3 of 60 took 2.004s
  training loss:		0.052887
Epoch 4 of 60 took 2.006s
  training loss:		0.038559
Epoch 5 of 60 took 2.008s
  training loss:		0.030909
Epoch 6 of 60 took 2.011s
  training loss:		0.026460
Epoch 7 of 60 took 2.007s
  training loss:		0.023497
Epoch 8 of 60 took 2.011s
  training loss:		0.021331
Epoch 9 of 60 took 2.006s
  training loss:		0.019629
Epoch 10 of 60 took 2.010s
  training loss:		0.018235
Epoch 11 of 60 took 2.016s
  training loss:		0.017117
Epoch 12 of 60 took 2.004s
  training loss:		0.016153
Epoch 13 of 60 took 2.011s
  training loss:		0.015295
Epoch 14 of 60 took 2.008s
  training loss:		0.014564
Epoch 15 of 60 took 2.008s
  training loss:		0.013911
Epoch 16 of 60 took 2.007s
  training loss:		0.013322
Epoch 17 of 60 took 2.015s
  training loss:		0.012765
Epoch 18 of 60 took 2.024s
  training loss:		0.012300
Epoch 19 of 60 took 2.027s
  training loss:		0.011882
Epoch 20 of 60 took 2.024s
  training loss:		0.011454
Epoch 21 of 60 took 2.030s
  training loss:		0.011101
Epoch 22 of 60 took 2.032s
  training loss:		0.010779
Epoch 23 of 60 took 2.023s
  training loss:		0.010488
Epoch 24 of 60 took 2.028s
  training loss:		0.010192
Epoch 25 of 60 took 2.030s
  training loss:		0.009948
Epoch 26 of 60 took 2.032s
  training loss:		0.009703
Epoch 27 of 60 took 2.030s
  training loss:		0.009479
Epoch 28 of 60 took 2.029s
  training loss:		0.009258
Epoch 29 of 60 took 2.026s
  training loss:		0.009075
Epoch 30 of 60 took 2.029s
  training loss:		0.008902
Epoch 31 of 60 took 2.034s
  training loss:		0.008719
Epoch 32 of 60 took 2.026s
  training loss:		0.008566
Epoch 33 of 60 took 2.030s
  training loss:		0.008418
Epoch 34 of 60 took 2.024s
  training loss:		0.008256
Epoch 35 of 60 took 2.027s
  training loss:		0.008118
Epoch 36 of 60 took 2.030s
  training loss:		0.008015
Epoch 37 of 60 took 2.028s
  training loss:		0.007861
Epoch 38 of 60 took 2.029s
  training loss:		0.007746
Epoch 39 of 60 took 2.027s
  training loss:		0.007637
Epoch 40 of 60 took 2.023s
  training loss:		0.007529
Epoch 41 of 60 took 2.023s
  training loss:		0.007432
Epoch 42 of 60 took 2.030s
  training loss:		0.007317
Epoch 43 of 60 took 2.026s
  training loss:		0.007221
Epoch 44 of 60 took 2.029s
  training loss:		0.007141
Epoch 45 of 60 took 2.029s
  training loss:		0.007055
Epoch 46 of 60 took 2.032s
  training loss:		0.006965
Epoch 47 of 60 took 2.022s
  training loss:		0.006870
Epoch 48 of 60 took 2.029s
  training loss:		0.006795
Epoch 49 of 60 took 2.032s
  training loss:		0.006725
Epoch 50 of 60 took 2.031s
  training loss:		0.006638
Epoch 51 of 60 took 2.035s
  training loss:		0.006574
Epoch 52 of 60 took 2.025s
  training loss:		0.006510
Epoch 53 of 60 took 2.026s
  training loss:		0.006429
Epoch 54 of 60 took 2.027s
  training loss:		0.006367
Epoch 55 of 60 took 2.030s
  training loss:		0.006304
Epoch 56 of 60 took 2.030s
  training loss:		0.006245
Epoch 57 of 60 took 2.030s
  training loss:		0.006184
Epoch 58 of 60 took 2.030s
  training loss:		0.006118
Epoch 59 of 60 took 2.028s
  training loss:		0.006055
Epoch 60 of 60 took 2.033s
  training loss:		0.006013
Training accuracy:		97.17 %
Final results:
  test loss:			0.005344
  test accuracy:		97.66 %
