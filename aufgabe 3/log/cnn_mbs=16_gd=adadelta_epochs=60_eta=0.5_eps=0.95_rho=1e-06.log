Loading MNIST dataset...
Creating network...
Using adadelta for updates with learning rate: 0.5, epsilon: 0.95, rho: 1e-06
Starting training...
Epoch 1 of 60 took 3.865s
  training loss:		0.014754
Epoch 2 of 60 took 3.793s
  training loss:		0.005550
Epoch 3 of 60 took 3.805s
  training loss:		0.004237
Epoch 4 of 60 took 3.799s
  training loss:		0.003571
Epoch 5 of 60 took 3.791s
  training loss:		0.003123
Epoch 6 of 60 took 3.799s
  training loss:		0.002753
Epoch 7 of 60 took 3.798s
  training loss:		0.002539
Epoch 8 of 60 took 3.797s
  training loss:		0.002300
Epoch 9 of 60 took 3.798s
  training loss:		0.002150
Epoch 10 of 60 took 3.795s
  training loss:		0.001990
Epoch 11 of 60 took 3.803s
  training loss:		0.001843
Epoch 12 of 60 took 3.798s
  training loss:		0.001748
Epoch 13 of 60 took 3.798s
  training loss:		0.001649
Epoch 14 of 60 took 3.794s
  training loss:		0.001580
Epoch 15 of 60 took 3.796s
  training loss:		0.001484
Epoch 16 of 60 took 3.794s
  training loss:		0.001399
Epoch 17 of 60 took 3.798s
  training loss:		0.001348
Epoch 18 of 60 took 3.792s
  training loss:		0.001276
Epoch 19 of 60 took 3.789s
  training loss:		0.001221
Epoch 20 of 60 took 3.782s
  training loss:		0.001160
Epoch 21 of 60 took 3.797s
  training loss:		0.001119
Epoch 22 of 60 took 3.797s
  training loss:		0.001066
Epoch 23 of 60 took 3.785s
  training loss:		0.001013
Epoch 24 of 60 took 3.794s
  training loss:		0.000979
Epoch 25 of 60 took 3.794s
  training loss:		0.000933
Epoch 26 of 60 took 3.795s
  training loss:		0.000881
Epoch 27 of 60 took 3.791s
  training loss:		0.000844
Epoch 28 of 60 took 3.798s
  training loss:		0.000826
Epoch 29 of 60 took 3.787s
  training loss:		0.000777
Epoch 30 of 60 took 3.786s
  training loss:		0.000778
Epoch 31 of 60 took 3.791s
  training loss:		0.000762
Epoch 32 of 60 took 3.788s
  training loss:		0.000700
Epoch 33 of 60 took 3.795s
  training loss:		0.000698
Epoch 34 of 60 took 3.792s
  training loss:		0.000668
Epoch 35 of 60 took 3.802s
  training loss:		0.000633
Epoch 36 of 60 took 3.794s
  training loss:		0.000609
Epoch 37 of 60 took 3.792s
  training loss:		0.000578
Epoch 38 of 60 took 3.801s
  training loss:		0.000564
Epoch 39 of 60 took 3.801s
  training loss:		0.000547
Epoch 40 of 60 took 3.803s
  training loss:		0.000555
Epoch 41 of 60 took 3.805s
  training loss:		0.000533
Epoch 42 of 60 took 3.801s
  training loss:		0.000513
Epoch 43 of 60 took 3.795s
  training loss:		0.000503
Epoch 44 of 60 took 3.799s
  training loss:		0.000472
Epoch 45 of 60 took 3.796s
  training loss:		0.000466
Epoch 46 of 60 took 3.799s
  training loss:		0.000466
Epoch 47 of 60 took 3.797s
  training loss:		0.000444
Epoch 48 of 60 took 3.806s
  training loss:		0.000421
Epoch 49 of 60 took 3.802s
  training loss:		0.000414
Epoch 50 of 60 took 3.800s
  training loss:		0.000395
Epoch 51 of 60 took 3.797s
  training loss:		0.000386
Epoch 52 of 60 took 3.800s
  training loss:		0.000378
Epoch 53 of 60 took 3.803s
  training loss:		0.000382
Epoch 54 of 60 took 3.798s
  training loss:		0.000370
Epoch 55 of 60 took 3.804s
  training loss:		0.000360
Epoch 56 of 60 took 3.803s
  training loss:		0.000354
Epoch 57 of 60 took 3.804s
  training loss:		0.000357
Epoch 58 of 60 took 3.805s
  training loss:		0.000346
Epoch 59 of 60 took 3.800s
  training loss:		0.000347
Epoch 60 of 60 took 3.800s
  training loss:		0.000338
Training accuracy:		99.74 %
Final results:
  test loss:			0.001693
  test accuracy:		99.32 %
