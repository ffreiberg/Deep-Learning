Loading MNIST dataset...
Creating network...
Using adadelta for updates with learning rate: 0.1, epsilon: 0.95, rho: 1e-06
Starting training...
Epoch 1 of 60 took 1.964s
  training loss:		0.105423
Epoch 2 of 60 took 1.923s
  training loss:		0.085385
Epoch 3 of 60 took 1.923s
  training loss:		0.078827
Epoch 4 of 60 took 1.925s
  training loss:		0.068377
Epoch 5 of 60 took 1.924s
  training loss:		0.055671
Epoch 6 of 60 took 1.923s
  training loss:		0.046001
Epoch 7 of 60 took 1.922s
  training loss:		0.040322
Epoch 8 of 60 took 1.925s
  training loss:		0.036418
Epoch 9 of 60 took 1.930s
  training loss:		0.032740
Epoch 10 of 60 took 1.928s
  training loss:		0.029365
Epoch 11 of 60 took 1.930s
  training loss:		0.026717
Epoch 12 of 60 took 1.929s
  training loss:		0.024637
Epoch 13 of 60 took 1.927s
  training loss:		0.022980
Epoch 14 of 60 took 1.931s
  training loss:		0.021579
Epoch 15 of 60 took 1.926s
  training loss:		0.020414
Epoch 16 of 60 took 1.928s
  training loss:		0.019405
Epoch 17 of 60 took 1.963s
  training loss:		0.018526
Epoch 18 of 60 took 1.927s
  training loss:		0.017768
Epoch 19 of 60 took 1.932s
  training loss:		0.017067
Epoch 20 of 60 took 1.929s
  training loss:		0.016467
Epoch 21 of 60 took 1.927s
  training loss:		0.015918
Epoch 22 of 60 took 1.930s
  training loss:		0.015411
Epoch 23 of 60 took 1.927s
  training loss:		0.014958
Epoch 24 of 60 took 1.927s
  training loss:		0.014539
Epoch 25 of 60 took 1.927s
  training loss:		0.014155
Epoch 26 of 60 took 1.926s
  training loss:		0.013812
Epoch 27 of 60 took 1.926s
  training loss:		0.013478
Epoch 28 of 60 took 1.931s
  training loss:		0.013147
Epoch 29 of 60 took 1.926s
  training loss:		0.012862
Epoch 30 of 60 took 1.927s
  training loss:		0.012601
Epoch 31 of 60 took 1.926s
  training loss:		0.012353
Epoch 32 of 60 took 1.926s
  training loss:		0.012106
Epoch 33 of 60 took 1.928s
  training loss:		0.011849
Epoch 34 of 60 took 1.925s
  training loss:		0.011664
Epoch 35 of 60 took 1.926s
  training loss:		0.011451
Epoch 36 of 60 took 1.926s
  training loss:		0.011242
Epoch 37 of 60 took 1.927s
  training loss:		0.011065
Epoch 38 of 60 took 1.927s
  training loss:		0.010878
Epoch 39 of 60 took 1.927s
  training loss:		0.010699
Epoch 40 of 60 took 1.927s
  training loss:		0.010528
Epoch 41 of 60 took 1.926s
  training loss:		0.010370
Epoch 42 of 60 took 1.929s
  training loss:		0.010232
Epoch 43 of 60 took 1.927s
  training loss:		0.010087
Epoch 44 of 60 took 1.927s
  training loss:		0.009931
Epoch 45 of 60 took 1.930s
  training loss:		0.009815
Epoch 46 of 60 took 1.928s
  training loss:		0.009685
Epoch 47 of 60 took 1.929s
  training loss:		0.009566
Epoch 48 of 60 took 1.928s
  training loss:		0.009452
Epoch 49 of 60 took 1.928s
  training loss:		0.009320
Epoch 50 of 60 took 1.929s
  training loss:		0.009206
Epoch 51 of 60 took 1.929s
  training loss:		0.009101
Epoch 52 of 60 took 1.926s
  training loss:		0.008995
Epoch 53 of 60 took 1.929s
  training loss:		0.008890
Epoch 54 of 60 took 1.929s
  training loss:		0.008802
Epoch 55 of 60 took 1.927s
  training loss:		0.008716
Epoch 56 of 60 took 1.931s
  training loss:		0.008622
Epoch 57 of 60 took 1.928s
  training loss:		0.008530
Epoch 58 of 60 took 1.929s
  training loss:		0.008447
Epoch 59 of 60 took 1.930s
  training loss:		0.008365
Epoch 60 of 60 took 1.929s
  training loss:		0.008281
Training accuracy:		96.10 %
Final results:
  test loss:			0.007223
  test accuracy:		96.92 %
