Loading MNIST dataset...
Creating network...
Using momentum for updates with learning rate: 0.1, momentum: 0.9
Starting training...
Epoch 1 of 60 took 2.069s
  training loss:		0.102092
Epoch 2 of 60 took 1.996s
  training loss:		0.100000
Epoch 3 of 60 took 1.995s
  training loss:		0.100000
Epoch 4 of 60 took 1.996s
  training loss:		0.100000
Epoch 5 of 60 took 1.997s
  training loss:		0.100000
Epoch 6 of 60 took 1.996s
  training loss:		0.100000
Epoch 7 of 60 took 1.995s
  training loss:		0.100000
Epoch 8 of 60 took 1.997s
  training loss:		0.100000
Epoch 9 of 60 took 2.002s
  training loss:		0.100000
Epoch 10 of 60 took 2.004s
  training loss:		0.100000
Epoch 11 of 60 took 2.001s
  training loss:		0.100000
Epoch 12 of 60 took 2.003s
  training loss:		0.100000
Epoch 13 of 60 took 2.003s
  training loss:		0.100000
Epoch 14 of 60 took 2.004s
  training loss:		0.100000
Epoch 15 of 60 took 2.003s
  training loss:		0.100000
Epoch 16 of 60 took 2.002s
  training loss:		0.100000
Epoch 17 of 60 took 2.003s
  training loss:		0.100000
Epoch 18 of 60 took 2.002s
  training loss:		0.100000
Epoch 19 of 60 took 2.003s
  training loss:		0.100000
Epoch 20 of 60 took 2.002s
  training loss:		0.100000
Epoch 21 of 60 took 2.003s
  training loss:		0.100000
Epoch 22 of 60 took 2.007s
  training loss:		0.100000
Epoch 23 of 60 took 2.009s
  training loss:		0.100000
Epoch 24 of 60 took 2.005s
  training loss:		0.100000
Epoch 25 of 60 took 2.005s
  training loss:		0.100000
Epoch 26 of 60 took 2.005s
  training loss:		0.100000
Epoch 27 of 60 took 2.005s
  training loss:		0.099999
Epoch 28 of 60 took 2.005s
  training loss:		0.099999
Epoch 29 of 60 took 2.005s
  training loss:		0.099999
Epoch 30 of 60 took 2.003s
  training loss:		0.099999
Epoch 31 of 60 took 2.007s
  training loss:		0.099999
Epoch 32 of 60 took 2.005s
  training loss:		0.099999
Epoch 33 of 60 took 2.005s
  training loss:		0.099999
Epoch 34 of 60 took 2.005s
  training loss:		0.099998
Epoch 35 of 60 took 2.006s
  training loss:		0.099998
Epoch 36 of 60 took 2.004s
  training loss:		0.099996
Epoch 37 of 60 took 2.006s
  training loss:		0.097718
Epoch 38 of 60 took 2.006s
  training loss:		0.092161
Epoch 39 of 60 took 2.009s
  training loss:		0.091475
Epoch 40 of 60 took 2.013s
  training loss:		0.087734
Epoch 41 of 60 took 2.012s
  training loss:		0.073923
Epoch 42 of 60 took 2.014s
  training loss:		0.053233
Epoch 43 of 60 took 2.017s
  training loss:		0.045580
Epoch 44 of 60 took 2.017s
  training loss:		0.032964
Epoch 45 of 60 took 2.017s
  training loss:		0.024523
Epoch 46 of 60 took 2.015s
  training loss:		0.023708
Epoch 47 of 60 took 2.017s
  training loss:		0.023320
Epoch 48 of 60 took 2.017s
  training loss:		0.023000
Epoch 49 of 60 took 2.020s
  training loss:		0.022779
Epoch 50 of 60 took 2.016s
  training loss:		0.022585
Epoch 51 of 60 took 2.016s
  training loss:		0.022407
Epoch 52 of 60 took 2.017s
  training loss:		0.022259
Epoch 53 of 60 took 2.017s
  training loss:		0.021937
Epoch 54 of 60 took 2.016s
  training loss:		0.013473
Epoch 55 of 60 took 2.015s
  training loss:		0.004759
Epoch 56 of 60 took 2.018s
  training loss:		0.003971
Epoch 57 of 60 took 2.017s
  training loss:		0.003645
Epoch 58 of 60 took 2.019s
  training loss:		0.003386
Epoch 59 of 60 took 2.016s
  training loss:		0.003196
Epoch 60 of 60 took 2.018s
  training loss:		0.003046
Training accuracy:		98.63 %
Final results:
  test loss:			0.003132
  test accuracy:		98.75 %
