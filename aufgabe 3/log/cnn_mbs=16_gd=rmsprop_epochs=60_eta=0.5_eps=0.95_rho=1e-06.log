Loading MNIST dataset...
Creating network...
Using rmsprop for updates with learning rate: 0.5, epsilon: 0.95, rho: 1e-06
Starting training...
Epoch 1 of 60 took 3.628s
  training loss:		0.013626
Epoch 2 of 60 took 3.557s
  training loss:		0.005031
Epoch 3 of 60 took 3.557s
  training loss:		0.003898
Epoch 4 of 60 took 3.556s
  training loss:		0.003316
Epoch 5 of 60 took 3.556s
  training loss:		0.002892
Epoch 6 of 60 took 3.556s
  training loss:		0.002594
Epoch 7 of 60 took 3.572s
  training loss:		0.002356
Epoch 8 of 60 took 3.578s
  training loss:		0.002156
Epoch 9 of 60 took 3.578s
  training loss:		0.001973
Epoch 10 of 60 took 3.577s
  training loss:		0.001850
Epoch 11 of 60 took 3.578s
  training loss:		0.001733
Epoch 12 of 60 took 3.579s
  training loss:		0.001617
Epoch 13 of 60 took 3.578s
  training loss:		0.001516
Epoch 14 of 60 took 3.580s
  training loss:		0.001412
Epoch 15 of 60 took 3.583s
  training loss:		0.001331
Epoch 16 of 60 took 3.577s
  training loss:		0.001243
Epoch 17 of 60 took 3.578s
  training loss:		0.001183
Epoch 18 of 60 took 3.579s
  training loss:		0.001136
Epoch 19 of 60 took 3.578s
  training loss:		0.001077
Epoch 20 of 60 took 3.578s
  training loss:		0.001048
Epoch 21 of 60 took 3.578s
  training loss:		0.000976
Epoch 22 of 60 took 3.577s
  training loss:		0.000915
Epoch 23 of 60 took 3.577s
  training loss:		0.000905
Epoch 24 of 60 took 3.577s
  training loss:		0.000843
Epoch 25 of 60 took 3.578s
  training loss:		0.000808
Epoch 26 of 60 took 3.578s
  training loss:		0.000754
Epoch 27 of 60 took 3.578s
  training loss:		0.000746
Epoch 28 of 60 took 3.577s
  training loss:		0.000731
Epoch 29 of 60 took 3.577s
  training loss:		0.000697
Epoch 30 of 60 took 3.577s
  training loss:		0.000657
Epoch 31 of 60 took 3.576s
  training loss:		0.000620
Epoch 32 of 60 took 3.582s
  training loss:		0.000601
Epoch 33 of 60 took 3.577s
  training loss:		0.000599
Epoch 34 of 60 took 3.578s
  training loss:		0.000555
Epoch 35 of 60 took 3.578s
  training loss:		0.000536
Epoch 36 of 60 took 3.578s
  training loss:		0.000527
Epoch 37 of 60 took 3.578s
  training loss:		0.000510
Epoch 38 of 60 took 3.578s
  training loss:		0.000490
Epoch 39 of 60 took 3.577s
  training loss:		0.000474
Epoch 40 of 60 took 3.577s
  training loss:		0.000467
Epoch 41 of 60 took 3.578s
  training loss:		0.000435
Epoch 42 of 60 took 3.576s
  training loss:		0.000428
Epoch 43 of 60 took 3.577s
  training loss:		0.000427
Epoch 44 of 60 took 3.577s
  training loss:		0.000418
Epoch 45 of 60 took 3.577s
  training loss:		0.000397
Epoch 46 of 60 took 3.577s
  training loss:		0.000389
Epoch 47 of 60 took 3.578s
  training loss:		0.000374
Epoch 48 of 60 took 3.576s
  training loss:		0.000378
Epoch 49 of 60 took 3.581s
  training loss:		0.000369
Epoch 50 of 60 took 3.576s
  training loss:		0.000364
Epoch 51 of 60 took 3.578s
  training loss:		0.000354
Epoch 52 of 60 took 3.577s
  training loss:		0.000339
Epoch 53 of 60 took 3.577s
  training loss:		0.000331
Epoch 54 of 60 took 3.578s
  training loss:		0.000335
Epoch 55 of 60 took 3.577s
  training loss:		0.000325
Epoch 56 of 60 took 3.577s
  training loss:		0.000325
Epoch 57 of 60 took 3.578s
  training loss:		0.000317
Epoch 58 of 60 took 3.577s
  training loss:		0.000312
Epoch 59 of 60 took 3.577s
  training loss:		0.000305
Epoch 60 of 60 took 3.578s
  training loss:		0.000305
Training accuracy:		99.78 %
Final results:
  test loss:			0.001890
  test accuracy:		99.22 %
