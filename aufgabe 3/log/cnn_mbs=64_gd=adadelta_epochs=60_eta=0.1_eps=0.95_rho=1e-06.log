Loading MNIST dataset...
Creating network...
Using adadelta for updates with learning rate: 0.1, epsilon: 0.95, rho: 1e-06
Starting training...
Epoch 1 of 60 took 2.374s
  training loss:		0.066884
Epoch 2 of 60 took 2.410s
  training loss:		0.023959
Epoch 3 of 60 took 2.324s
  training loss:		0.016813
Epoch 4 of 60 took 2.340s
  training loss:		0.013788
Epoch 5 of 60 took 2.266s
  training loss:		0.011868
Epoch 6 of 60 took 2.266s
  training loss:		0.010466
Epoch 7 of 60 took 2.262s
  training loss:		0.009452
Epoch 8 of 60 took 2.260s
  training loss:		0.008655
Epoch 9 of 60 took 2.264s
  training loss:		0.008012
Epoch 10 of 60 took 2.278s
  training loss:		0.007519
Epoch 11 of 60 took 2.282s
  training loss:		0.007119
Epoch 12 of 60 took 2.390s
  training loss:		0.006761
Epoch 13 of 60 took 2.435s
  training loss:		0.006464
Epoch 14 of 60 took 2.283s
  training loss:		0.006190
Epoch 15 of 60 took 2.340s
  training loss:		0.005978
Epoch 16 of 60 took 2.379s
  training loss:		0.005771
Epoch 17 of 60 took 2.324s
  training loss:		0.005581
Epoch 18 of 60 took 2.300s
  training loss:		0.005404
Epoch 19 of 60 took 2.487s
  training loss:		0.005248
Epoch 20 of 60 took 2.310s
  training loss:		0.005102
Epoch 21 of 60 took 2.358s
  training loss:		0.004976
Epoch 22 of 60 took 2.335s
  training loss:		0.004847
Epoch 23 of 60 took 2.318s
  training loss:		0.004744
Epoch 24 of 60 took 2.327s
  training loss:		0.004640
Epoch 25 of 60 took 2.396s
  training loss:		0.004547
Epoch 26 of 60 took 2.395s
  training loss:		0.004450
Epoch 27 of 60 took 2.312s
  training loss:		0.004356
Epoch 28 of 60 took 2.337s
  training loss:		0.004266
Epoch 29 of 60 took 2.279s
  training loss:		0.004187
Epoch 30 of 60 took 2.318s
  training loss:		0.004117
Epoch 31 of 60 took 2.313s
  training loss:		0.004055
Epoch 32 of 60 took 2.309s
  training loss:		0.003977
Epoch 33 of 60 took 2.308s
  training loss:		0.003917
Epoch 34 of 60 took 2.280s
  training loss:		0.003849
Epoch 35 of 60 took 2.283s
  training loss:		0.003782
Epoch 36 of 60 took 2.339s
  training loss:		0.003727
Epoch 37 of 60 took 2.360s
  training loss:		0.003686
Epoch 38 of 60 took 2.279s
  training loss:		0.003617
Epoch 39 of 60 took 2.396s
  training loss:		0.003559
Epoch 40 of 60 took 2.407s
  training loss:		0.003517
Epoch 41 of 60 took 2.395s
  training loss:		0.003470
Epoch 42 of 60 took 2.286s
  training loss:		0.003426
Epoch 43 of 60 took 2.384s
  training loss:		0.003388
Epoch 44 of 60 took 2.338s
  training loss:		0.003343
Epoch 45 of 60 took 2.359s
  training loss:		0.003299
Epoch 46 of 60 took 2.452s
  training loss:		0.003250
Epoch 47 of 60 took 2.299s
  training loss:		0.003228
Epoch 48 of 60 took 2.283s
  training loss:		0.003177
Epoch 49 of 60 took 2.317s
  training loss:		0.003167
Epoch 50 of 60 took 2.397s
  training loss:		0.003117
Epoch 51 of 60 took 2.448s
  training loss:		0.003081
Epoch 52 of 60 took 2.365s
  training loss:		0.003046
Epoch 53 of 60 took 2.370s
  training loss:		0.003008
Epoch 54 of 60 took 2.430s
  training loss:		0.002988
Epoch 55 of 60 took 2.289s
  training loss:		0.002953
Epoch 56 of 60 took 2.276s
  training loss:		0.002911
Epoch 57 of 60 took 2.300s
  training loss:		0.002885
Epoch 58 of 60 took 2.402s
  training loss:		0.002875
Epoch 59 of 60 took 2.424s
  training loss:		0.002841
Epoch 60 of 60 took 2.449s
  training loss:		0.002819
Training accuracy:		98.72 %
Final results:
  test loss:			0.002865
  test accuracy:		98.71 %
