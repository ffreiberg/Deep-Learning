Loading MNIST dataset...
Creating network...
Using momentum for updates with learning rate: 0.1, momentum: 0.9
Starting training...
Epoch 1 of 60 took 2.258s
  training loss:		0.100978
Epoch 2 of 60 took 2.192s
  training loss:		0.098668
Epoch 3 of 60 took 2.193s
  training loss:		0.089784
Epoch 4 of 60 took 2.194s
  training loss:		0.089437
Epoch 5 of 60 took 2.189s
  training loss:		0.089315
Epoch 6 of 60 took 2.190s
  training loss:		0.089234
Epoch 7 of 60 took 2.189s
  training loss:		0.089190
Epoch 8 of 60 took 2.189s
  training loss:		0.089155
Epoch 9 of 60 took 2.206s
  training loss:		0.089131
Epoch 10 of 60 took 2.235s
  training loss:		0.089073
Epoch 11 of 60 took 2.279s
  training loss:		0.049349
Epoch 12 of 60 took 2.257s
  training loss:		0.007567
Epoch 13 of 60 took 2.393s
  training loss:		0.005552
Epoch 14 of 60 took 2.398s
  training loss:		0.004576
Epoch 15 of 60 took 2.398s
  training loss:		0.003977
Epoch 16 of 60 took 2.392s
  training loss:		0.003586
Epoch 17 of 60 took 2.192s
  training loss:		0.003282
Epoch 18 of 60 took 2.191s
  training loss:		0.003004
Epoch 19 of 60 took 2.190s
  training loss:		0.002824
Epoch 20 of 60 took 2.191s
  training loss:		0.002644
Epoch 21 of 60 took 2.191s
  training loss:		0.002516
Epoch 22 of 60 took 2.190s
  training loss:		0.002355
Epoch 23 of 60 took 2.191s
  training loss:		0.002286
Epoch 24 of 60 took 2.190s
  training loss:		0.002137
Epoch 25 of 60 took 2.190s
  training loss:		0.002081
Epoch 26 of 60 took 2.190s
  training loss:		0.001992
Epoch 27 of 60 took 2.194s
  training loss:		0.001894
Epoch 28 of 60 took 2.190s
  training loss:		0.001828
Epoch 29 of 60 took 2.190s
  training loss:		0.001737
Epoch 30 of 60 took 2.190s
  training loss:		0.001704
Epoch 31 of 60 took 2.190s
  training loss:		0.001628
Epoch 32 of 60 took 2.191s
  training loss:		0.001570
Epoch 33 of 60 took 2.189s
  training loss:		0.001518
Epoch 34 of 60 took 2.190s
  training loss:		0.001464
Epoch 35 of 60 took 2.190s
  training loss:		0.001435
Epoch 36 of 60 took 2.190s
  training loss:		0.001410
Epoch 37 of 60 took 2.190s
  training loss:		0.001330
Epoch 38 of 60 took 2.193s
  training loss:		0.001313
Epoch 39 of 60 took 2.194s
  training loss:		0.001267
Epoch 40 of 60 took 2.196s
  training loss:		0.001223
Epoch 41 of 60 took 2.194s
  training loss:		0.001220
Epoch 42 of 60 took 2.194s
  training loss:		0.001163
Epoch 43 of 60 took 2.192s
  training loss:		0.001141
Epoch 44 of 60 took 2.189s
  training loss:		0.001130
Epoch 45 of 60 took 2.190s
  training loss:		0.001075
Epoch 46 of 60 took 2.190s
  training loss:		0.001057
Epoch 47 of 60 took 2.190s
  training loss:		0.001037
Epoch 48 of 60 took 2.190s
  training loss:		0.001009
Epoch 49 of 60 took 2.190s
  training loss:		0.000987
Epoch 50 of 60 took 2.195s
  training loss:		0.000927
Epoch 51 of 60 took 2.193s
  training loss:		0.000917
Epoch 52 of 60 took 2.194s
  training loss:		0.000891
Epoch 53 of 60 took 2.194s
  training loss:		0.000892
Epoch 54 of 60 took 2.199s
  training loss:		0.000849
Epoch 55 of 60 took 2.196s
  training loss:		0.000828
Epoch 56 of 60 took 2.196s
  training loss:		0.000800
Epoch 57 of 60 took 2.196s
  training loss:		0.000787
Epoch 58 of 60 took 2.196s
  training loss:		0.000789
Epoch 59 of 60 took 2.195s
  training loss:		0.000778
Epoch 60 of 60 took 2.195s
  training loss:		0.000745
Training accuracy:		99.64 %
Final results:
  test loss:			0.001936
  test accuracy:		99.23 %
